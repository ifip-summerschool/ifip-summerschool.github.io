---
title: "Keynotes"
permalink: /keynotes/
toc: true
toc_sticky: false
---

### Keynote Speakers
We are happy to announce the following keynote speakers for the 20th IFIP Summer School on Privacy and Identity Management (listed in alphabetical order).

<!-- ----------------------------------------------------- -->
<hr id="nicolas">
<img src="../assets/images/keynote_speakers/nicolas_diaz_ferreyra.jpg" alt=" Nicol√°s E. D√≠az Ferreyra" style="float: left; width: 100px; height: 100px; position: relative; border: 2px solid white; -webkit-border-radius: 50px; -moz-border-radius: 50px; border-radius: 50px; overflow:hidden;"/>[ Nicol√°s E. D√≠az Ferreyra](https://www.ndiaz-ferreyra.com/)
<p style="margin-top:-20px; font-size: 70%;">üá©üá™ Hamburg University of Technology, Institute of Software Security</p>
<p style="text-align: justify;"><strong>Title:</strong> Privacy as Practice: Perspectives from Users and Developer-Centred Research</p>
<p style="text-align: justify;"><strong>Abstract:</strong> Information disclosure is essential to many digital services, ranging from social networks to critical domains like healthcare and finance. Yet privacy decisions are often based on spurious or incomplete assessments of risk, which often translate into unwanted incidents for end users. This challenge has been addressed from multiple angles, including psychology, information systems, and law. Furthermore, prior work has elaborated extensively on preventative technologies that aim at supporting users‚Äô privacy choices through behavioural interventions or ‚Äúnudges‚Äù. However, much less attention has been paid to privacy decision-making within software development processes. Particularly, to the personal and project-sensitive information exchanged by practitioners as they deploy software-intensive systems, and the associated risks. In this talk, I will share some lessons I have learned while investigating self-disclosure practices in social networks and elaborate on how they can help us shape a research agenda for privacy in software development environments. I will also review the current state of the art on privacy engineering methods and discuss prospective research directions in the area of privacy ‚Äòas code‚Äô.</p>
<!-- ----------------------------------------------------- -->
<hr id="agnieszka">
<img src="../assets/images/keynote_speakers/agnieszka_kitkowska.jpg" alt="Agnieszka Kitkowska" style="float: left; width: 100px; height: 100px; position: relative; border: 2px solid white; -webkit-border-radius: 50px; -moz-border-radius: 50px; border-radius: 50px; overflow:hidden;"/>[Agnieszka Kitkowska](https://ju.se/personinfo.html?sign=KITAGN&lang=en)
<p style="margin-top:-20px; font-size: 70%;">üá∏üá™ J√∂nk√∂ping University,  Department of Computer Science and Informatics , School of Engineering</p>
<p style="text-align: justify;"><strong>Title:</strong> What We Assume, What We Miss: Rethinking How We Understand and Design for Privacy</p>
<p style="text-align: justify;"><strong>Abstract:</strong> Designing technical solutions that aim to help people make more informed decisions about their privacy might be challenging. To make such tools usable and relevant to individuals, one must understand factors that affect people's behavior. Privacy research in the past has revealed many of these factors, such as privacy concerns, personality characteristics, or previous experiences. However, when designing for privacy, certain assumptions are often made, such as stereotyping or generalizing about the privacy consequences. In this talk, we will discuss how such approaches might be misleading and how modern norm-critical approaches, such as intersectionality, can help gain a deeper understanding of the privacy-related decisions made by various individuals or groups. </p>
<!-- ----------------------------------------------------- -->
<hr id="anja">
<img src="../assets/images/keynote_speakers/anja_moller_pedersen.jpg" alt="Anja M√∏ller Pedersen" style="float: left; width: 100px; height: 100px; position: relative; border: 2px solid white; -webkit-border-radius: 50px; -moz-border-radius: 50px; border-radius: 50px; overflow:hidden;"/>[Anja M√∏ller Pedersen](https://www.humanrights.dk/staff/anja-moller-pedersen)
<p style="margin-top:-20px; font-size: 70%;">üá©üá∞ Danish Institute for Human Rights &amp; University of Copenhagen, Faculty of Law</p>
<p style="text-align: justify;"><strong>Title:</strong> Introducing Facial Recognition Technology for Law Enforcement Purposes in Denmark: Parliamentary Support Without Democratic Legitimacy</p>
<p style="text-align: justify;"><strong>Abstract:</strong> In 2024, the Danish Parliament gave its support to the police to start using facial recognition technology (FRT) for law enforcement purposes. However, the exact scope and conditions for use remain unknown to the public as the use of FRT has no distinct legal basis but is based on the general competences of the police to investigate criminal activities and process personal data. The keynote explores this interference with the fundamental rights in Art 8 ECHR and Arts 7 and 8 CFREU against the requirements of lawfulness in Art 8(2) ECHR and ‚Äòstrict necessity‚Äô in relation to Art 8 CFREU and discusses more generally the ways in which digital investigative tools are regulated with varying levels of detail and democratic scrutiny. </p>
<!-- ----------------------------------------------------- -->
<hr id="katherine">
<img src="../assets/images/keynote_speakers/katherine_quezada.jpg" alt="Katherine Quezada" style="float: left; width: 100px; height: 100px; position: relative; border: 2px solid white; -webkit-border-radius: 50px; -moz-border-radius: 50px; border-radius: 50px; overflow:hidden;"/>
[Katherine Quezada Tav√°rez](https://www.linkedin.com/in/katherine-quezada/)
<p style="margin-top:-20px; font-size: 70%;">üáßüá™ UZ Leuven</p>
<p style="text-align: justify;"><strong>Title:</strong> Building Bridges in Privacy and Data Protection: A Practitioner‚Äôs Tale of Interdisciplinary Work</p>
<p style="text-align: justify;"><strong>Abstract:</strong> How do lawyers, engineers, security professionals, data scientists, ethicists, business leaders, and clinicians actually work together on privacy and data protection? And what happens when they do not? Drawing on an enduring trajectory at the intersection of law, ethics, compliance, research, and technology, this keynote explores the practical realities of a data protection career across academic, corporate, and healthcare settings. It dives into the human complexity of privacy governance, where disciplinary divides, competing priorities, and communication gaps can hinder progress, but also where genuine collaboration can drive meaningful change. Through lived examples and professional insights, the talk reflects on what it takes to build functional, trusted, and resilient data protection cultures in practice. 
</p>
<!-- ----------------------------------------------------- -->
<hr id="andrej">
<img src="../assets/images/keynote_speakers/andrej_savin.jpg" alt="Andrej Savin" style="float: left; width: 100px; height: 100px; position: relative; border: 2px solid white; -webkit-border-radius: 50px; -moz-border-radius: 50px; border-radius: 50px; overflow:hidden;"/>[Andrej Savin](https://www.cbs.dk/en/research/departments-and-centres/department-of-business-humanities-and-law/staff/asjur)
<p style="margin-top:-20px; font-size: 70%;">üá©üá∞ Copenhagen Business School, Department of Business Humanities and Law</p>
<p style="text-align: justify;"><strong>Title:</strong> Navigating AI and Data Privacy: Clarifying the Relationship between the AI Act and GDPR</p>
<p style="text-align: justify;"><strong>Abstract:</strong> The Artificial Intelligence Act (AI Act) is a comprehensive regulatory framework designed to govern Artificial Intelligence (AI) within the European Union (EU) through risk-based, ex ante horizontal product-safety rules. Artificial Intelligence (AI) systems heavily rely on data sets for training, and these data sets are subject to the General Data Protection Regulation (GDPR) in the extent that they are based on personal data. This interplay between GDPR and the AI Act raises several compliance challenges. This presentation provides an overview of the primary issues pertaining to the relationship between the GDPR and the AI Act. We will delve into matters such as the scope of application, the legal hierarchy, the allocation of responsibility, and the legal foundations of compliance. The overarching concept is that adhering to GDPR compliance may often prove to be the more onerous of the two regulatory frameworks. 
</p>
<!-- ----------------------------------------------------- -->
<hr id="paul">
<img src="../assets/images/keynote_speakers/paul_stankovski_wagner.jpg" alt="Paul Stankovski Wagner" style="float: left; width: 100px; height: 100px; position: relative; border: 2px solid white; -webkit-border-radius: 50px; -moz-border-radius: 50px; border-radius: 50px; overflow:hidden;"/>
[Paul Stankovski Wagner](https://portal.research.lu.se/en/persons/paul-stankovski-wagner)
<p style="margin-top:-20px; font-size: 70%;">üá∏üá™ Lund University, Department of Electrical and Information Technology</p>
<p style="text-align: justify;"><strong>Title:</strong> Digitalization & PAPR: Publicly Auditable Privacy Revocation for Anonymous Credentials</p>
<p style="text-align: justify;"><strong>Abstract:</strong> Digitalization is a popular word nowadays, but what is digitalization really, is it just fluff or is it concretely useful? Digitalization can help us to build better systems, technical systems with new properties, but to see how and why, we need to first talk about some basics. An engineering interpretation of digitalization, and how it is changing engineering education at Lund University.
PAPR is a technical system that provides auditability to privacy revocation systems. Simply explained, think of a centralized system in which the system owner holds all user data and can apply mass surveillance at will. With PAPR techniques, we decentralize user data to build a system that cannot be used for mass surveillance without the users knowing about it. That is, if the system is mis-used, we (users) will know about it. This brings a whole different level of (potential) public trust into a proprietary system, letting the users audit the system they are using. A small step towards achieving and maintaining privacy and data protection in a rapidly changing world, aimer at higher objectives in future technical systems. The PAPR paper was published at CT-RSA in 2023.
 
<!-- ----------------------------------------------------- -->

